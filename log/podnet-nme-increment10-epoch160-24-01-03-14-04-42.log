Label: podnet_nme_cifar100_xsteps
orders : None
{'model': 'podnet', 'convnet': 'rebuffi', 'dropout': 0.0, 'herding': None, 'memory_size': 2000, 'temperature': 1, 'fixed_memory': True, 'dataset': 'cifar100', 'increment': 10, 'batch_size': 128, 'workers': 0, 'threads': 1, 'validation': 0.0, 'random_classes': False, 'max_task': None, 'onehot': False, 'initial_increment': 50, 'sampler': None, 'data_path': '/data/douillard/', 'lr': 0.1, 'weight_decay': 0.0005, 'scheduling': 'cosine', 'lr_decay': 0.1, 'optimizer': 'sgd', 'epochs': 160, 'device': [0], 'label': 'podnet_nme_cifar100_xsteps', 'autolabel': False, 'seed': [1], 'seed_range': None, 'options': None, 'save_model': 'last', 'dump_predictions': False, 'logging': 'info', 'resume': None, 'resume_first': False, 'recompute_meta': False, 'no_benchmark': False, 'detect_anomaly': False, 'dummy': 1, 'includes': ['headers/dummy.yaml'], 'data_root': 'D:/data/douillard/cifar100/cifar100', 'save_path': '.', 'eval_type': 'nme', 'backbone': {'name': 'resnet18'}, 'classifier': {'name': 'PODNet'}, 'classifier_config': {'type': 'cosine', 'proxy_per_class': 10, 'distance': 'neg_stable_cosine_distance'}, 'postprocessor_config': {'type': 'learned_scaling', 'initial_value': 1.0}, 'pod_flat': {'scheduled_factor': 1.0}, 'pod_spatial': {'scheduled_factor': 3.0, 'collapse_channels': 'spatial'}, 'nca': {'margin': 0.6, 'scale': 1.0, 'exclude_pos_denominator': True}, 'groupwise_factors': {'old_weights': 0.0}, 'finetuning_config': {'sampling': 'undersampling', 'tuning': 'classifier', 'lr': 0.05, 'epochs': 20, 'scaling': None}, 'proxy_per_class': 1, 'weight_generation': {'type': 'imprinted', 'multi_class_diff': 'kmeans'}, 'dataset_transforms': {'color_jitter': True}}
Launching run 1/1
Set seed 1
CUDA algos are determinists but very slow!
Files already downloaded and verified
Files already downloaded and verified
Dataset iCIFAR100: class ordering: [87, 0, 52, 58, 44, 91, 68, 97, 51, 15, 94, 92, 10, 72, 49, 78, 61, 14, 8, 86, 84, 96, 18, 24, 32, 45, 88, 11, 4, 67, 69, 66, 77, 47, 79, 93, 29, 50, 57, 83, 17, 81, 41, 12, 37, 59, 25, 20, 80, 73, 1, 28, 6, 46, 62, 82, 53, 9, 31, 75, 38, 63, 33, 74, 27, 22, 36, 3, 16, 21, 60, 19, 70, 90, 89, 43, 5, 42, 65, 76, 40, 30, 23, 85, 2, 95, 56, 48, 71, 64, 98, 13, 99, 7, 34, 55, 54, 26, 35, 39].
Downsampling type stride
Using 10 proxies per class.
Model will be save at this rythm: last.
================Task 0 Start!================
Testing on False unseen tasks (max class = 50).
Before task
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 0 Training!================
The training samples number: 25000
Train on 0->50.
train task
nb 25000.
Train [1/6] | Epoch [1/160] |	nca: 713.351884841919, loss: 713.351884841919 
Train [1/6] | Epoch [2/160] |	nca: 628.6165583133698, loss: 628.6165583133698 
Train [1/6] | Epoch [3/160] |	nca: 578.6331541538239, loss: 578.6331541538239 
Train [1/6] | Epoch [4/160] |	nca: 530.7386615276337, loss: 530.7386615276337 
Train [1/6] | Epoch [5/160] |	nca: 479.500501871109, loss: 479.500501871109 
Train [1/6] | Epoch [6/160] |	nca: 435.3442208766937, loss: 435.3442208766937 
Train [1/6] | Epoch [7/160] |	nca: 401.26643419265747, loss: 401.26643419265747 
Train [1/6] | Epoch [8/160] |	nca: 375.8024117946625, loss: 375.8024117946625 
Train [1/6] | Epoch [9/160] |	nca: 354.27213966846466, loss: 354.27213966846466 
Train [1/6] | Epoch [10/160] |	nca: 336.8898422718048, loss: 336.8898422718048 
Train [1/6] | Epoch [11/160] |	nca: 320.44804549217224, loss: 320.44804549217224 
Train [1/6] | Epoch [12/160] |	nca: 305.83493304252625, loss: 305.83493304252625 
Train [1/6] | Epoch [13/160] |	nca: 294.6501029729843, loss: 294.6501029729843 
Train [1/6] | Epoch [14/160] |	nca: 282.2315227985382, loss: 282.2315227985382 
Train [1/6] | Epoch [15/160] |	nca: 274.926295876503, loss: 274.926295876503 
Train [1/6] | Epoch [16/160] |	nca: 266.0881495475769, loss: 266.0881495475769 
Train [1/6] | Epoch [17/160] |	nca: 259.16981571912766, loss: 259.16981571912766 
Train [1/6] | Epoch [18/160] |	nca: 254.72574758529663, loss: 254.72574758529663 
Train [1/6] | Epoch [19/160] |	nca: 248.95968079566956, loss: 248.95968079566956 
Train [1/6] | Epoch [20/160] |	nca: 243.20548540353775, loss: 243.20548540353775 
Train [1/6] | Epoch [21/160] |	nca: 239.14421570301056, loss: 239.14421570301056 
Train [1/6] | Epoch [22/160] |	nca: 231.69573068618774, loss: 231.69573068618774 
Train [1/6] | Epoch [23/160] |	nca: 230.83445185422897, loss: 230.83445185422897 
Train [1/6] | Epoch [24/160] |	nca: 224.39093899726868, loss: 224.39093899726868 
Train [1/6] | Epoch [25/160] |	nca: 223.75408631563187, loss: 223.75408631563187 
Train [1/6] | Epoch [26/160] |	nca: 220.56728547811508, loss: 220.56728547811508 
Train [1/6] | Epoch [27/160] |	nca: 217.50402510166168, loss: 217.50402510166168 
Train [1/6] | Epoch [28/160] |	nca: 214.038755774498, loss: 214.038755774498 
Train [1/6] | Epoch [29/160] |	nca: 211.85838121175766, loss: 211.85838121175766 
Train [1/6] | Epoch [30/160] |	nca: 209.0929234623909, loss: 209.0929234623909 
Train [1/6] | Epoch [31/160] |	nca: 204.9795206785202, loss: 204.9795206785202 
Train [1/6] | Epoch [32/160] |	nca: 206.8096480369568, loss: 206.8096480369568 
Train [1/6] | Epoch [33/160] |	nca: 204.11862134933472, loss: 204.11862134933472 
Train [1/6] | Epoch [34/160] |	nca: 200.40510100126266, loss: 200.40510100126266 
Train [1/6] | Epoch [35/160] |	nca: 198.49675315618515, loss: 198.49675315618515 
Train [1/6] | Epoch [36/160] |	nca: 196.67374467849731, loss: 196.67374467849731 
Train [1/6] | Epoch [37/160] |	nca: 194.41171395778656, loss: 194.41171395778656 
Train [1/6] | Epoch [38/160] |	nca: 191.68678677082062, loss: 191.68678677082062 
Train [1/6] | Epoch [39/160] |	nca: 192.71395355463028, loss: 192.71395355463028 
Train [1/6] | Epoch [40/160] |	nca: 190.81061494350433, loss: 190.81061494350433 
Train [1/6] | Epoch [41/160] |	nca: 189.1604282259941, loss: 189.1604282259941 
Train [1/6] | Epoch [42/160] |	nca: 186.20672190189362, loss: 186.20672190189362 
Train [1/6] | Epoch [43/160] |	nca: 186.92845010757446, loss: 186.92845010757446 
Train [1/6] | Epoch [44/160] |	nca: 181.49871999025345, loss: 181.49871999025345 
Train [1/6] | Epoch [45/160] |	nca: 181.77578687667847, loss: 181.77578687667847 
Train [1/6] | Epoch [46/160] |	nca: 181.2056167125702, loss: 181.2056167125702 
Train [1/6] | Epoch [47/160] |	nca: 181.16315454244614, loss: 181.16315454244614 
Train [1/6] | Epoch [48/160] |	nca: 178.53908595442772, loss: 178.53908595442772 
Train [1/6] | Epoch [49/160] |	nca: 175.36670005321503, loss: 175.36670005321503 
Train [1/6] | Epoch [50/160] |	nca: 174.0537033677101, loss: 174.0537033677101 
Train [1/6] | Epoch [51/160] |	nca: 171.19595915079117, loss: 171.19595915079117 
Train [1/6] | Epoch [52/160] |	nca: 172.69342017173767, loss: 172.69342017173767 
Train [1/6] | Epoch [53/160] |	nca: 168.82566076517105, loss: 168.82566076517105 
Train [1/6] | Epoch [54/160] |	nca: 167.9023533463478, loss: 167.9023533463478 
Train [1/6] | Epoch [55/160] |	nca: 167.24974232912064, loss: 167.24974232912064 
Train [1/6] | Epoch [56/160] |	nca: 165.85436302423477, loss: 165.85436302423477 
Train [1/6] | Epoch [57/160] |	nca: 165.5051113963127, loss: 165.5051113963127 
Train [1/6] | Epoch [58/160] |	nca: 164.60466295480728, loss: 164.60466295480728 
Train [1/6] | Epoch [59/160] |	nca: 160.6327401995659, loss: 160.6327401995659 
Train [1/6] | Epoch [60/160] |	nca: 162.23893290758133, loss: 162.23893290758133 
Train [1/6] | Epoch [61/160] |	nca: 157.18538808822632, loss: 157.18538808822632 
Train [1/6] | Epoch [62/160] |	nca: 156.47558665275574, loss: 156.47558665275574 
Train [1/6] | Epoch [63/160] |	nca: 154.64292865991592, loss: 154.64292865991592 
Train [1/6] | Epoch [64/160] |	nca: 155.6742660999298, loss: 155.6742660999298 
Train [1/6] | Epoch [65/160] |	nca: 149.62107729911804, loss: 149.62107729911804 
Train [1/6] | Epoch [66/160] |	nca: 153.21525579690933, loss: 153.21525579690933 
Train [1/6] | Epoch [67/160] |	nca: 147.12253135442734, loss: 147.12253135442734 
Train [1/6] | Epoch [68/160] |	nca: 146.77572679519653, loss: 146.77572679519653 
Train [1/6] | Epoch [69/160] |	nca: 147.21071964502335, loss: 147.21071964502335 
Train [1/6] | Epoch [70/160] |	nca: 143.65168660879135, loss: 143.65168660879135 
Train [1/6] | Epoch [71/160] |	nca: 145.1826254427433, loss: 145.1826254427433 
Train [1/6] | Epoch [72/160] |	nca: 140.94649094343185, loss: 140.94649094343185 
Train [1/6] | Epoch [73/160] |	nca: 136.1016357243061, loss: 136.1016357243061 
Train [1/6] | Epoch [74/160] |	nca: 136.10018265247345, loss: 136.10018265247345 
Train [1/6] | Epoch [75/160] |	nca: 137.39468535780907, loss: 137.39468535780907 
Train [1/6] | Epoch [76/160] |	nca: 134.91196206212044, loss: 134.91196206212044 
Train [1/6] | Epoch [77/160] |	nca: 133.55890649557114, loss: 133.55890649557114 
Train [1/6] | Epoch [78/160] |	nca: 130.7498186826706, loss: 130.7498186826706 
Train [1/6] | Epoch [79/160] |	nca: 129.03210878372192, loss: 129.03210878372192 
Train [1/6] | Epoch [80/160] |	nca: 127.07343074679375, loss: 127.07343074679375 
Train [1/6] | Epoch [81/160] |	nca: 126.50620111823082, loss: 126.50620111823082 
Train [1/6] | Epoch [82/160] |	nca: 125.5404884815216, loss: 125.5404884815216 
Train [1/6] | Epoch [83/160] |	nca: 121.37834912538528, loss: 121.37834912538528 
Train [1/6] | Epoch [84/160] |	nca: 120.21516990661621, loss: 120.21516990661621 
Train [1/6] | Epoch [85/160] |	nca: 118.00880980491638, loss: 118.00880980491638 
Train [1/6] | Epoch [86/160] |	nca: 122.26294612884521, loss: 122.26294612884521 
Train [1/6] | Epoch [87/160] |	nca: 114.48825865983963, loss: 114.48825865983963 
Train [1/6] | Epoch [88/160] |	nca: 112.44232502579689, loss: 112.44232502579689 
Train [1/6] | Epoch [89/160] |	nca: 113.65240815281868, loss: 113.65240815281868 
Train [1/6] | Epoch [90/160] |	nca: 108.59288465976715, loss: 108.59288465976715 
Train [1/6] | Epoch [91/160] |	nca: 107.31433835625648, loss: 107.31433835625648 
Train [1/6] | Epoch [92/160] |	nca: 106.03064024448395, loss: 106.03064024448395 
Train [1/6] | Epoch [93/160] |	nca: 104.8274872303009, loss: 104.8274872303009 
Train [1/6] | Epoch [94/160] |	nca: 99.3927595615387, loss: 99.3927595615387 
Train [1/6] | Epoch [95/160] |	nca: 98.01765170693398, loss: 98.01765170693398 
Train [1/6] | Epoch [96/160] |	nca: 98.72503092885017, loss: 98.72503092885017 
Train [1/6] | Epoch [97/160] |	nca: 92.91395708918571, loss: 92.91395708918571 
Train [1/6] | Epoch [98/160] |	nca: 94.99951711297035, loss: 94.99951711297035 
Train [1/6] | Epoch [99/160] |	nca: 90.99919417500496, loss: 90.99919417500496 
Train [1/6] | Epoch [100/160] |	nca: 89.08166259527206, loss: 89.08166259527206 
Train [1/6] | Epoch [101/160] |	nca: 87.35527998209, loss: 87.35527998209 
Train [1/6] | Epoch [102/160] |	nca: 84.42908239364624, loss: 84.42908239364624 
Train [1/6] | Epoch [103/160] |	nca: 81.95249280333519, loss: 81.95249280333519 
Train [1/6] | Epoch [104/160] |	nca: 78.56588837504387, loss: 78.56588837504387 
Train [1/6] | Epoch [105/160] |	nca: 77.78540995717049, loss: 77.78540995717049 
Train [1/6] | Epoch [106/160] |	nca: 77.76533743739128, loss: 77.76533743739128 
Train [1/6] | Epoch [107/160] |	nca: 73.34200575947762, loss: 73.34200575947762 
Train [1/6] | Epoch [108/160] |	nca: 67.52499675750732, loss: 67.52499675750732 
Train [1/6] | Epoch [109/160] |	nca: 67.94865590333939, loss: 67.94865590333939 
Train [1/6] | Epoch [110/160] |	nca: 68.80205161869526, loss: 68.80205161869526 
Train [1/6] | Epoch [111/160] |	nca: 63.13556386530399, loss: 63.13556386530399 
Train [1/6] | Epoch [112/160] |	nca: 63.32496574521065, loss: 63.32496574521065 
Train [1/6] | Epoch [113/160] |	nca: 57.91928431391716, loss: 57.91928431391716 
Train [1/6] | Epoch [114/160] |	nca: 58.13551875948906, loss: 58.13551875948906 
Train [1/6] | Epoch [115/160] |	nca: 55.22845076024532, loss: 55.22845076024532 
Train [1/6] | Epoch [116/160] |	nca: 51.329284861683846, loss: 51.329284861683846 
Train [1/6] | Epoch [117/160] |	nca: 46.37196624279022, loss: 46.37196624279022 
Train [1/6] | Epoch [118/160] |	nca: 45.11584118753672, loss: 45.11584118753672 
Train [1/6] | Epoch [119/160] |	nca: 44.9192823022604, loss: 44.9192823022604 
Train [1/6] | Epoch [120/160] |	nca: 41.42254064977169, loss: 41.42254064977169 
Train [1/6] | Epoch [121/160] |	nca: 37.90351743251085, loss: 37.90351743251085 
Train [1/6] | Epoch [122/160] |	nca: 37.02498168870807, loss: 37.02498168870807 
Train [1/6] | Epoch [123/160] |	nca: 37.104259222745895, loss: 37.104259222745895 
Train [1/6] | Epoch [124/160] |	nca: 32.7702416703105, loss: 32.7702416703105 
Train [1/6] | Epoch [125/160] |	nca: 30.432747825980186, loss: 30.432747825980186 
Train [1/6] | Epoch [126/160] |	nca: 28.43459513783455, loss: 28.43459513783455 
Train [1/6] | Epoch [127/160] |	nca: 26.454925294965506, loss: 26.454925294965506 
Train [1/6] | Epoch [128/160] |	nca: 24.065714728087187, loss: 24.065714728087187 
Train [1/6] | Epoch [129/160] |	nca: 23.70870105549693, loss: 23.70870105549693 
Train [1/6] | Epoch [130/160] |	nca: 21.023952335119247, loss: 21.023952335119247 
Train [1/6] | Epoch [131/160] |	nca: 19.12814335897565, loss: 19.12814335897565 
Train [1/6] | Epoch [132/160] |	nca: 17.830847334116697, loss: 17.830847334116697 
Train [1/6] | Epoch [133/160] |	nca: 16.285766765475273, loss: 16.285766765475273 
Train [1/6] | Epoch [134/160] |	nca: 14.202484237030149, loss: 14.202484237030149 
