Label: podnet_cnn_cifar100_50steps
orders : None
{'model': 'podnet', 'convnet': 'rebuffi', 'dropout': 0.0, 'herding': None, 'memory_size': 2000, 'temperature': 1, 'fixed_memory': True, 'dataset': 'cifar100', 'increment': 5, 'batch_size': 128, 'workers': 0, 'threads': 1, 'validation': 0.0, 'random_classes': False, 'max_task': None, 'onehot': False, 'initial_increment': 50, 'sampler': None, 'data_path': '/data/douillard/', 'lr': 0.1, 'weight_decay': 0.0005, 'scheduling': 'cosine', 'lr_decay': 0.1, 'optimizer': 'sgd', 'epochs': 20, 'device': [0], 'label': 'podnet_cnn_cifar100_50steps', 'autolabel': False, 'seed': [1], 'seed_range': None, 'options': ['config/podnet_cnn_cifar100.yaml'], 'save_model': 'last', 'dump_predictions': False, 'logging': 'info', 'resume': None, 'resume_first': False, 'recompute_meta': False, 'no_benchmark': False, 'detect_anomaly': False, 'includes': ['headers/data.yaml', 'headers/device.yaml', 'headers/model.yaml', 'headers/optimizer.yaml', 'backbones/resnet12.yaml'], 'data_root': 'D:/data/douillard/cifar100/cifar100', 'save_path': '.', 'init_cls_num': 10, 'inc_cls_num': 10, 'task_num': 10, 'epoch': 100, 'device_ids': 0, 'n_gpu': 1, 'val_per_epoch': 10, 'backbone': {'name': 'resnet18'}, 'classifier': {'name': 'PODNet'}, 'eval_type': 'cnn', 'classifier_config': {'type': 'cosine', 'proxy_per_class': 10, 'distance': 'neg_stable_cosine_distance'}, 'postprocessor_config': {'type': 'learned_scaling', 'initial_value': 1.0}, 'pod_flat': {'scheduled_factor': 1.0}, 'pod_spatial': {'scheduled_factor': 3.0, 'collapse_channels': 'spatial'}, 'nca': {'margin': 0.6, 'scale': 1.0, 'exclude_pos_denominator': True}, 'groupwise_factors': {'old_weights': 0.0}, 'finetuning_config': {'sampling': 'undersampling', 'tuning': 'classifier', 'lr': 0.05, 'epochs': 20, 'scaling': None}, 'proxy_per_class': 1, 'weight_generation': {'type': 'imprinted', 'multi_class_diff': 'kmeans'}, 'dataset_transforms': {'color_jitter': True}}
Launching run 1/1
Set seed 1
CUDA algos are determinists but very slow!
Files already downloaded and verified
Files already downloaded and verified
Dataset iCIFAR100: class ordering: [87, 0, 52, 58, 44, 91, 68, 97, 51, 15, 94, 92, 10, 72, 49, 78, 61, 14, 8, 86, 84, 96, 18, 24, 32, 45, 88, 11, 4, 67, 69, 66, 77, 47, 79, 93, 29, 50, 57, 83, 17, 81, 41, 12, 37, 59, 25, 20, 80, 73, 1, 28, 6, 46, 62, 82, 53, 9, 31, 75, 38, 63, 33, 74, 27, 22, 36, 3, 16, 21, 60, 19, 70, 90, 89, 43, 5, 42, 65, 76, 40, 30, 23, 85, 2, 95, 56, 48, 71, 64, 98, 13, 99, 7, 34, 55, 54, 26, 35, 39].
Downsampling type stride
Using 10 proxies per class.
model :  <core.model.replay.podnet.PODNet object at 0x000001C9AF846FD0>
Model will be save at this rythm: last.
================Task 0 Start!================
Testing on False unseen tasks (max class = 50).
Before task
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 0 Training!================
The training samples number: 25000
Train on 0->50.
train task
nb 25000.
Train [1/11] | Epoch [1/20] |	nca: 713.351884841919, loss: 713.351884841919 
Train [1/11] | Epoch [2/20] |	nca: 628.6165583133698, loss: 628.6165583133698 
Train [1/11] | Epoch [3/20] |	nca: 578.3092842102051, loss: 578.3092842102051 
Train [1/11] | Epoch [4/20] |	nca: 530.577987909317, loss: 530.577987909317 
Train [1/11] | Epoch [5/20] |	nca: 478.8509452342987, loss: 478.8509452342987 
Train [1/11] | Epoch [6/20] |	nca: 433.16194462776184, loss: 433.16194462776184 
Train [1/11] | Epoch [7/20] |	nca: 396.6498050689697, loss: 396.6498050689697 
Train [1/11] | Epoch [8/20] |	nca: 369.76339077949524, loss: 369.76339077949524 
Train [1/11] | Epoch [9/20] |	nca: 344.0063245296478, loss: 344.0063245296478 
Train [1/11] | Epoch [10/20] |	nca: 320.4426773786545, loss: 320.4426773786545 
Train [1/11] | Epoch [11/20] |	nca: 299.91507959365845, loss: 299.91507959365845 
Train [1/11] | Epoch [12/20] |	nca: 277.965349316597, loss: 277.965349316597 
Train [1/11] | Epoch [13/20] |	nca: 262.2289720773697, loss: 262.2289720773697 
Train [1/11] | Epoch [14/20] |	nca: 241.5601927638054, loss: 241.5601927638054 
Train [1/11] | Epoch [15/20] |	nca: 223.61824983358383, loss: 223.61824983358383 
Train [1/11] | Epoch [16/20] |	nca: 206.26379543542862, loss: 206.26379543542862 
Train [1/11] | Epoch [17/20] |	nca: 188.299909055233, loss: 188.299909055233 
Train [1/11] | Epoch [18/20] |	nca: 170.82645279169083, loss: 170.82645279169083 
Train [1/11] | Epoch [19/20] |	nca: 158.54840606451035, loss: 158.54840606451035 
Train [1/11] | Epoch [20/20] |	nca: 149.03298938274384, loss: 149.03298938274384 
after task
Building & updating memory.
after task
Eval on 0->50.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.701.
Current acc: {'total': 0.701, '00-09': 0.75, '10-19': 0.714, '20-29': 0.68, '30-39': 0.647, '40-49': 0.715}.
Avg inc acc top5: 0.926.
Current acc top5: {'total': 0.926}.
Forgetting: 0.0.
Cord metric: 0.70.
================Task 1 Start!================
Testing on False unseen tasks (max class = 55).
Set memory of size: 1000.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 1 Training!================
The training samples number: 3500
Train on 50->55.
train task
nb 3500.
Train [2/11] | Epoch [1/20] |	nca: 27.917473554611206, flat: 8.202380925416946, pod: 67.31425547599792, loss: 103.43410897254944 
Train [2/11] | Epoch [2/20] |	nca: 22.111920595169067, flat: 8.705534964799881, pod: 67.5058343410492, loss: 98.32328963279724 
Train [2/11] | Epoch [3/20] |	nca: 19.105883359909058, flat: 7.011917263269424, pod: 59.95891976356506, loss: 86.07672071456909 
Train [2/11] | Epoch [4/20] |	nca: 16.97840306162834, flat: 6.334219515323639, pod: 56.569851875305176, loss: 79.88247442245483 
Train [2/11] | Epoch [5/20] |	nca: 15.062409818172455, flat: 5.5661428570747375, pod: 52.77695143222809, loss: 73.40550470352173 
Train [2/11] | Epoch [6/20] |	nca: 14.62528932094574, flat: 5.356424391269684, pod: 51.238118052482605, loss: 71.21983170509338 
Train [2/11] | Epoch [7/20] |	nca: 14.382210552692413, flat: 5.294400990009308, pod: 50.96593415737152, loss: 70.64254570007324 
Train [2/11] | Epoch [8/20] |	nca: 12.91791832447052, flat: 4.8577903509140015, pod: 48.72739636898041, loss: 66.50310492515564 
Train [2/11] | Epoch [9/20] |	nca: 12.384166926145554, flat: 4.508148238062859, pod: 46.034778475761414, loss: 62.927093505859375 
Train [2/11] | Epoch [10/20] |	nca: 11.158394277095795, flat: 4.139443598687649, pod: 44.437082409858704, loss: 59.73491966724396 
Train [2/11] | Epoch [11/20] |	nca: 10.63793632388115, flat: 3.696969151496887, pod: 41.44597017765045, loss: 55.78087556362152 
Train [2/11] | Epoch [12/20] |	nca: 10.441135734319687, flat: 3.3565563559532166, pod: 37.73077189922333, loss: 51.5284640789032 
Train [2/11] | Epoch [13/20] |	nca: 9.962987780570984, flat: 3.2520786225795746, pod: 37.38590371608734, loss: 50.60097002983093 
Train [2/11] | Epoch [14/20] |	nca: 9.630836218595505, flat: 2.940248504281044, pod: 35.45263612270355, loss: 48.02372062206268 
Train [2/11] | Epoch [15/20] |	nca: 8.643950551748276, flat: 2.658111073076725, pod: 32.69135928153992, loss: 43.99342107772827 
Train [2/11] | Epoch [16/20] |	nca: 8.113493114709854, flat: 2.328204683959484, pod: 30.187905251979828, loss: 40.629602909088135 
Train [2/11] | Epoch [17/20] |	nca: 8.371494054794312, flat: 2.280514992773533, pod: 29.35551232099533, loss: 40.00752115249634 
Train [2/11] | Epoch [18/20] |	nca: 8.336798340082169, flat: 2.1586335748434067, pod: 28.73004025220871, loss: 39.22547209262848 
Train [2/11] | Epoch [19/20] |	nca: 7.866309180855751, flat: 2.021487895399332, pod: 26.94863396883011, loss: 36.83643090724945 
Train [2/11] | Epoch [20/20] |	nca: 7.962978199124336, flat: 1.9423371851444244, pod: 25.42280811071396, loss: 35.328123331069946 
Fine-tuning
Building & updating memory.
Train [2/11] | Epoch [21/40] |	nca: 7.923950433731079, flat: 1.5209780186414719, pod: 13.704651594161987, loss: 23.149579763412476 
Train [2/11] | Epoch [22/40] |	nca: 4.846013516187668, flat: 1.528048798441887, pod: 13.703868865966797, loss: 20.07793116569519 
Train [2/11] | Epoch [23/40] |	nca: 3.9665797352790833, flat: 1.5900956243276596, pod: 13.921977519989014, loss: 19.47865319252014 
Train [2/11] | Epoch [24/40] |	nca: 3.3195511400699615, flat: 1.5229486525058746, pod: 13.468418955802917, loss: 18.310918927192688 
Train [2/11] | Epoch [25/40] |	nca: 3.002376616001129, flat: 1.52691450715065, pod: 13.633463263511658, loss: 18.16275429725647 
Train [2/11] | Epoch [26/40] |	nca: 2.9261943101882935, flat: 1.5191249400377274, pod: 13.722517013549805, loss: 18.16783630847931 
Train [2/11] | Epoch [27/40] |	nca: 2.824480950832367, flat: 1.530486561357975, pod: 13.769620895385742, loss: 18.12458848953247 
Train [2/11] | Epoch [28/40] |	nca: 2.4058632254600525, flat: 1.527008056640625, pod: 13.583263516426086, loss: 17.51613473892212 
Train [2/11] | Epoch [29/40] |	nca: 2.503800854086876, flat: 1.5546218901872635, pod: 14.08266282081604, loss: 18.141085624694824 
Train [2/11] | Epoch [30/40] |	nca: 2.3162753582000732, flat: 1.5252434760332108, pod: 13.747406721115112, loss: 17.58892548084259 
Train [2/11] | Epoch [31/40] |	nca: 2.174283742904663, flat: 1.5548144429922104, pod: 13.882825255393982, loss: 17.611923456192017 
Train [2/11] | Epoch [32/40] |	nca: 2.1136891841888428, flat: 1.5646188333630562, pod: 13.952532768249512, loss: 17.63084089756012 
Train [2/11] | Epoch [33/40] |	nca: 2.205029621720314, flat: 1.538722112774849, pod: 13.761511206626892, loss: 17.505262970924377 
Train [2/11] | Epoch [34/40] |	nca: 2.037278935313225, flat: 1.5124434530735016, pod: 13.585870742797852, loss: 17.13559317588806 
Train [2/11] | Epoch [35/40] |	nca: 2.036841794848442, flat: 1.5408936440944672, pod: 13.785216927528381, loss: 17.36295235157013 
Train [2/11] | Epoch [36/40] |	nca: 1.9552189707756042, flat: 1.48645581305027, pod: 13.313897132873535, loss: 16.755571961402893 
Train [2/11] | Epoch [37/40] |	nca: 1.9011369049549103, flat: 1.5205281227827072, pod: 13.852062225341797, loss: 17.27372717857361 
Train [2/11] | Epoch [38/40] |	nca: 1.7538720518350601, flat: 1.5396156013011932, pod: 13.710591197013855, loss: 17.00407874584198 
Train [2/11] | Epoch [39/40] |	nca: 1.8149408847093582, flat: 1.5430487543344498, pod: 13.745433330535889, loss: 17.10342276096344 
Train [2/11] | Epoch [40/40] |	nca: 1.7547643035650253, flat: 1.530951589345932, pod: 13.664543151855469, loss: 16.9502592086792 
after task
Building & updating memory.
after task
Eval on 0->55.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.6725.
Current acc: {'total': 0.644, '00-09': 0.701, '10-19': 0.629, '20-29': 0.597, '30-39': 0.594, '40-49': 0.675, '50-59': 0.694}.
Avg inc acc top5: 0.9115.
Current acc top5: {'total': 0.897}.
Forgetting: -0.05485714285714284.
Cord metric: 0.67.
Old accuracy: 0.64, mean: 0.64.
New accuracy: 0.69, mean: 0.69.
================Task 2 Start!================
Testing on False unseen tasks (max class = 60).
Set memory of size: 1100.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 2 Training!================
The training samples number: 3600
Train on 55->60.
train task
nb 3600.
Train [3/11] | Epoch [1/20] |	nca: 18.798864126205444, flat: 4.369458865374327, pod: 50.294841289520264, loss: 73.46316421031952 
Train [3/11] | Epoch [2/20] |	nca: 17.221374481916428, flat: 7.108985066413879, pod: 63.39346504211426, loss: 87.72382426261902 
Train [3/11] | Epoch [3/20] |	nca: 13.65657576918602, flat: 5.787812903523445, pod: 56.64867091178894, loss: 76.09306001663208 
Train [3/11] | Epoch [4/20] |	nca: 12.037350177764893, flat: 5.223092705011368, pod: 52.726439237594604, loss: 69.98688197135925 
Train [3/11] | Epoch [5/20] |	nca: 12.336636692285538, flat: 6.0104947835206985, pod: 57.807066202163696, loss: 76.15419793128967 
Train [3/11] | Epoch [6/20] |	nca: 10.933357551693916, flat: 5.173490971326828, pod: 53.56719255447388, loss: 69.67404079437256 
Train [3/11] | Epoch [7/20] |	nca: 9.965497866272926, flat: 4.8441076427698135, pod: 52.056270599365234, loss: 66.86587572097778 
Train [3/11] | Epoch [8/20] |	nca: 8.298449486494064, flat: 4.034402295947075, pod: 46.0623517036438, loss: 58.395203948020935 
Train [3/11] | Epoch [9/20] |	nca: 7.326916605234146, flat: 3.4165297001600266, pod: 42.633182406425476, loss: 53.37662851810455 
Train [3/11] | Epoch [10/20] |	nca: 7.156136140227318, flat: 3.361691437661648, pod: 43.0094256401062, loss: 53.52725327014923 
Train [3/11] | Epoch [11/20] |	nca: 7.163778588175774, flat: 3.197012387216091, pod: 40.4090473651886, loss: 50.76983880996704 
Train [3/11] | Epoch [12/20] |	nca: 6.21684493124485, flat: 2.7455365136265755, pod: 37.793474078178406, loss: 46.755855321884155 
Train [3/11] | Epoch [13/20] |	nca: 6.273250907659531, flat: 2.6393104195594788, pod: 36.59347224235535, loss: 45.50603365898132 
Train [3/11] | Epoch [14/20] |	nca: 5.874621540307999, flat: 2.3391514271497726, pod: 34.06599986553192, loss: 42.27977275848389 
Train [3/11] | Epoch [15/20] |	nca: 5.487784959375858, flat: 2.148136354982853, pod: 31.24742817878723, loss: 38.88334929943085 
Train [3/11] | Epoch [16/20] |	nca: 5.178426347672939, flat: 1.9991717599332333, pod: 29.650829792022705, loss: 36.82842803001404 
Train [3/11] | Epoch [17/20] |	nca: 5.129510216414928, flat: 1.8095942325890064, pod: 27.1767635345459, loss: 34.11586797237396 
Train [3/11] | Epoch [18/20] |	nca: 4.65393739938736, flat: 1.6380945183336735, pod: 26.17555582523346, loss: 32.467587888240814 
Train [3/11] | Epoch [19/20] |	nca: 4.96904231607914, flat: 1.526242721825838, pod: 24.93931084871292, loss: 31.434595704078674 
Train [3/11] | Epoch [20/20] |	nca: 4.423225626349449, flat: 1.5042751803994179, pod: 24.584103286266327, loss: 30.511603891849518 
Fine-tuning
Building & updating memory.
Train [3/11] | Epoch [21/40] |	nca: 4.03248593211174, flat: 1.0905900374054909, pod: 13.897696614265442, loss: 19.020772576332092 
Train [3/11] | Epoch [22/40] |	nca: 2.512900933623314, flat: 1.0708143785595894, pod: 13.914881587028503, loss: 17.498597025871277 
Train [3/11] | Epoch [23/40] |	nca: 2.089355245232582, flat: 1.0756746083498, pod: 13.661986589431763, loss: 16.827016353607178 
Train [3/11] | Epoch [24/40] |	nca: 2.1168244183063507, flat: 1.0779971852898598, pod: 13.990055322647095, loss: 17.184877157211304 
Train [3/11] | Epoch [25/40] |	nca: 1.9258244335651398, flat: 1.0604800581932068, pod: 13.808728814125061, loss: 16.79503345489502 
Train [3/11] | Epoch [26/40] |	nca: 1.9025953561067581, flat: 1.0752931535243988, pod: 13.951232433319092, loss: 16.929120898246765 
Train [3/11] | Epoch [27/40] |	nca: 1.7364521771669388, flat: 1.0505847483873367, pod: 13.852204084396362, loss: 16.639240980148315 
Train [3/11] | Epoch [28/40] |	nca: 1.7946439683437347, flat: 1.0875730365514755, pod: 14.095954060554504, loss: 16.9781711101532 
Train [3/11] | Epoch [29/40] |	nca: 1.7022837549448013, flat: 1.0951423346996307, pod: 13.981407284736633, loss: 16.778833389282227 
Train [3/11] | Epoch [30/40] |	nca: 1.5922721773386002, flat: 1.0848295390605927, pod: 13.51953125, loss: 16.196633100509644 
Train [3/11] | Epoch [31/40] |	nca: 1.5839966535568237, flat: 1.1020491644740105, pod: 14.010968327522278, loss: 16.697014212608337 
Train [3/11] | Epoch [32/40] |	nca: 1.526506707072258, flat: 1.1143305078148842, pod: 14.191880226135254, loss: 16.832717299461365 
Train [3/11] | Epoch [33/40] |	nca: 1.5944930016994476, flat: 1.1289179399609566, pod: 13.981789350509644, loss: 16.70520043373108 
Train [3/11] | Epoch [34/40] |	nca: 1.4692174717783928, flat: 1.100704126060009, pod: 13.688213109970093, loss: 16.258134603500366 
Train [3/11] | Epoch [35/40] |	nca: 1.4633846133947372, flat: 1.1325191408395767, pod: 14.618640065193176, loss: 17.21454393863678 
Train [3/11] | Epoch [36/40] |	nca: 1.4861895740032196, flat: 1.114313818514347, pod: 14.007213234901428, loss: 16.60771667957306 
Train [3/11] | Epoch [37/40] |	nca: 1.4298835918307304, flat: 1.0889206603169441, pod: 13.732107758522034, loss: 16.250912189483643 
Train [3/11] | Epoch [38/40] |	nca: 1.421065703034401, flat: 1.0862085744738579, pod: 13.898987889289856, loss: 16.406262159347534 
Train [3/11] | Epoch [39/40] |	nca: 1.436718925833702, flat: 1.0634024366736412, pod: 13.88605785369873, loss: 16.386179089546204 
Train [3/11] | Epoch [40/40] |	nca: 1.3790086582303047, flat: 1.1199298724532127, pod: 14.133647084236145, loss: 16.632585644721985 
after task
Building & updating memory.
after task
Eval on 0->60.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.654.
Current acc: {'total': 0.617, '00-09': 0.657, '10-19': 0.569, '20-29': 0.546, '30-39': 0.568, '40-49': 0.647, '50-59': 0.716}.
Avg inc acc top5: 0.9006666666666666.
Current acc top5: {'total': 0.879}.
Forgetting: 0.071.
Cord metric: 0.66.
Old accuracy: 0.60, mean: 0.62.
New accuracy: 0.81, mean: 0.75.
================Task 3 Start!================
Testing on False unseen tasks (max class = 65).
Set memory of size: 1200.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 3 Training!================
The training samples number: 3700
Train on 60->65.
train task
nb 3700.
Train [4/11] | Epoch [1/20] |	nca: 29.71681720018387, flat: 3.95473912358284, pod: 47.3027720451355, loss: 80.97432804107666 
Train [4/11] | Epoch [2/20] |	nca: 22.53932023048401, flat: 4.017211943864822, pod: 48.294803738594055, loss: 74.85133576393127 
Train [4/11] | Epoch [3/20] |	nca: 19.864942014217377, flat: 3.733982779085636, pod: 46.74510872364044, loss: 70.3440330028534 
Train [4/11] | Epoch [4/20] |	nca: 18.415297031402588, flat: 3.5328998118638992, pod: 46.018269538879395, loss: 67.9664659500122 
Train [4/11] | Epoch [5/20] |	nca: 16.746366530656815, flat: 3.431993506848812, pod: 44.29614746570587, loss: 64.47450757026672 
Train [4/11] | Epoch [6/20] |	nca: 15.988772600889206, flat: 3.3721985518932343, pod: 43.561617493629456, loss: 62.92258834838867 
Train [4/11] | Epoch [7/20] |	nca: 15.085103124380112, flat: 3.231661908328533, pod: 41.78044593334198, loss: 60.09721040725708 
Train [4/11] | Epoch [8/20] |	nca: 14.629728376865387, flat: 3.265546776354313, pod: 44.043914437294006, loss: 61.939189434051514 
Train [4/11] | Epoch [9/20] |	nca: 13.21718654036522, flat: 2.977432206273079, pod: 40.3555543422699, loss: 56.55017328262329 
Train [4/11] | Epoch [10/20] |	nca: 12.813613384962082, flat: 2.8492833077907562, pod: 38.916566371917725, loss: 54.579463601112366 
Train [4/11] | Epoch [11/20] |	nca: 12.277239948511124, flat: 2.730882979929447, pod: 37.371095299720764, loss: 52.379218339920044 
Train [4/11] | Epoch [12/20] |	nca: 11.610464870929718, flat: 2.531857803463936, pod: 36.669793128967285, loss: 50.8121155500412 
Train [4/11] | Epoch [13/20] |	nca: 10.505016773939133, flat: 2.25623432546854, pod: 33.23119467496872, loss: 45.992446064949036 
Train [4/11] | Epoch [14/20] |	nca: 10.415945500135422, flat: 2.1114086359739304, pod: 32.53574055433273, loss: 45.06309461593628 
Train [4/11] | Epoch [15/20] |	nca: 10.165550321340561, flat: 2.1867496222257614, pod: 32.456029653549194, loss: 44.808329701423645 
Train [4/11] | Epoch [16/20] |	nca: 10.025875061750412, flat: 1.8829081058502197, pod: 29.530052959918976, loss: 41.438836336135864 
Train [4/11] | Epoch [17/20] |	nca: 9.30185529589653, flat: 1.8039460219442844, pod: 28.458690285682678, loss: 39.564491629600525 
Train [4/11] | Epoch [18/20] |	nca: 9.26211592555046, flat: 1.671375673264265, pod: 26.465762853622437, loss: 37.39925456047058 
Train [4/11] | Epoch [19/20] |	nca: 8.646736025810242, flat: 1.660171926021576, pod: 26.669411778450012, loss: 36.97632014751434 
Train [4/11] | Epoch [20/20] |	nca: 8.5167206376791, flat: 1.5575411431491375, pod: 24.99276554584503, loss: 35.06702744960785 
Fine-tuning
Building & updating memory.
Train [4/11] | Epoch [21/40] |	nca: 6.246916055679321, flat: 1.9937474504113197, pod: 18.22323989868164, loss: 26.463903665542603 
Train [4/11] | Epoch [22/40] |	nca: 3.014838993549347, flat: 1.8786699399352074, pod: 17.34510326385498, loss: 22.238612294197083 
Train [4/11] | Epoch [23/40] |	nca: 3.055697053670883, flat: 2.0071733817458153, pod: 18.394513845443726, loss: 23.45738434791565 
Train [4/11] | Epoch [24/40] |	nca: 2.3015653043985367, flat: 1.8446928560733795, pod: 17.076467037200928, loss: 21.22272539138794 
Train [4/11] | Epoch [25/40] |	nca: 2.659030318260193, flat: 1.9192004352807999, pod: 17.52217996120453, loss: 22.10041081905365 
Train [4/11] | Epoch [26/40] |	nca: 1.9747928828001022, flat: 1.8972724378108978, pod: 17.21949338912964, loss: 21.091558694839478 
Train [4/11] | Epoch [27/40] |	nca: 2.213694527745247, flat: 1.9025249779224396, pod: 17.608728647232056, loss: 21.724948167800903 
Train [4/11] | Epoch [28/40] |	nca: 1.8533297628164291, flat: 1.8643727749586105, pod: 17.211416840553284, loss: 20.929119348526 
Train [4/11] | Epoch [29/40] |	nca: 1.8769888579845428, flat: 1.9449446350336075, pod: 17.902271509170532, loss: 21.724205017089844 
Train [4/11] | Epoch [30/40] |	nca: 1.8835384100675583, flat: 1.8698393106460571, pod: 17.099052786827087, loss: 20.85243058204651 
Train [4/11] | Epoch [31/40] |	nca: 1.8072203248739243, flat: 1.933890923857689, pod: 17.608129382133484, loss: 21.349241137504578 
Train [4/11] | Epoch [32/40] |	nca: 1.6615196540951729, flat: 1.879025638103485, pod: 17.257115960121155, loss: 20.797661185264587 
Train [4/11] | Epoch [33/40] |	nca: 1.776741050183773, flat: 1.8687500059604645, pod: 17.619778037071228, loss: 21.265268921852112 
Train [4/11] | Epoch [34/40] |	nca: 1.6618467047810555, flat: 1.867249310016632, pod: 17.19535267353058, loss: 20.724448680877686 
Train [4/11] | Epoch [35/40] |	nca: 1.8247362822294235, flat: 1.9847260266542435, pod: 17.80356216430664, loss: 21.613024711608887 
Train [4/11] | Epoch [36/40] |	nca: 1.8675594627857208, flat: 2.0381707847118378, pod: 18.058412194252014, loss: 21.964142441749573 
Train [4/11] | Epoch [37/40] |	nca: 1.6433713510632515, flat: 1.9484164640307426, pod: 17.621285796165466, loss: 21.21307373046875 
Train [4/11] | Epoch [38/40] |	nca: 1.9500249028205872, flat: 1.922443464398384, pod: 17.90590274333954, loss: 21.778371214866638 
Train [4/11] | Epoch [39/40] |	nca: 1.5183676034212112, flat: 1.942081943154335, pod: 17.55844211578369, loss: 21.01889169216156 
Train [4/11] | Epoch [40/40] |	nca: 1.9131248965859413, flat: 1.9708091765642166, pod: 17.703596115112305, loss: 21.587530255317688 
after task
Building & updating memory.
after task
Eval on 0->65.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.6345.
Current acc: {'total': 0.576, '00-09': 0.626, '10-19': 0.557, '20-29': 0.48, '30-39': 0.526, '40-49': 0.611, '50-59': 0.658, '60-69': 0.576}.
Avg inc acc top5: 0.889.
Current acc top5: {'total': 0.854}.
Forgetting: 0.023499999999999993.
Cord metric: 0.63.
Old accuracy: 0.58, mean: 0.60.
New accuracy: 0.58, mean: 0.69.
================Task 4 Start!================
Testing on False unseen tasks (max class = 70).
Set memory of size: 1300.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 4 Training!================
The training samples number: 3800
Train on 65->70.
train task
nb 3800.
Train [5/11] | Epoch [1/20] |	nca: 26.73473173379898, flat: 3.5868019349873066, pod: 45.51451599597931, loss: 75.83605003356934 
Train [5/11] | Epoch [2/20] |	nca: 19.814572244882584, flat: 3.623255155980587, pod: 47.47820043563843, loss: 70.91602754592896 
Train [5/11] | Epoch [3/20] |	nca: 17.489024251699448, flat: 3.55758036673069, pod: 48.34288823604584, loss: 69.38949394226074 
Train [5/11] | Epoch [4/20] |	nca: 14.95457273721695, flat: 3.0731345117092133, pod: 43.73960888385773, loss: 61.76731622219086 
Train [5/11] | Epoch [5/20] |	nca: 14.751578688621521, flat: 3.0639465674757957, pod: 42.915300488471985, loss: 60.730826020240784 
Train [5/11] | Epoch [6/20] |	nca: 14.621451318264008, flat: 3.3588426038622856, pod: 45.54370105266571, loss: 63.52399504184723 
Train [5/11] | Epoch [7/20] |	nca: 13.297126948833466, flat: 3.0558721646666527, pod: 42.743258237838745, loss: 59.0962575674057 
Train [5/11] | Epoch [8/20] |	nca: 12.467013865709305, flat: 2.8243120685219765, pod: 40.692052245140076, loss: 55.98337769508362 
Train [5/11] | Epoch [9/20] |	nca: 11.467279464006424, flat: 2.7271267622709274, pod: 40.29042637348175, loss: 54.484832644462585 
Train [5/11] | Epoch [10/20] |	nca: 10.334560453891754, flat: 2.3047852218151093, pod: 36.56884777545929, loss: 49.20819318294525 
Train [5/11] | Epoch [11/20] |	nca: 9.861737459897995, flat: 2.3353383019566536, pod: 35.82040500640869, loss: 48.01748049259186 
Train [5/11] | Epoch [12/20] |	nca: 9.723304867744446, flat: 2.0939963161945343, pod: 34.151578307151794, loss: 45.96887958049774 
Train [5/11] | Epoch [13/20] |	nca: 9.27770148217678, flat: 2.130306877195835, pod: 34.39605152606964, loss: 45.804059743881226 
Train [5/11] | Epoch [14/20] |	nca: 8.171860843896866, flat: 1.790902316570282, pod: 30.031238555908203, loss: 39.99400186538696 
Train [5/11] | Epoch [15/20] |	nca: 8.049777463078499, flat: 1.7078430689871311, pod: 29.12014365196228, loss: 38.877763867378235 
Train [5/11] | Epoch [16/20] |	nca: 7.493026912212372, flat: 1.5873223654925823, pod: 27.468283593654633, loss: 36.54863226413727 
Train [5/11] | Epoch [17/20] |	nca: 7.447695344686508, flat: 1.5150630846619606, pod: 26.043045163154602, loss: 35.005803406238556 
Train [5/11] | Epoch [18/20] |	nca: 7.253080382943153, flat: 1.4251884557306767, pod: 24.6950044631958, loss: 33.37327325344086 
Train [5/11] | Epoch [19/20] |	nca: 7.1061456352472305, flat: 1.3437961861491203, pod: 23.634562492370605, loss: 32.08450424671173 
Train [5/11] | Epoch [20/20] |	nca: 7.003752365708351, flat: 1.3947510607540607, pod: 23.69778949022293, loss: 32.09629291296005 
Fine-tuning
Building & updating memory.
Train [5/11] | Epoch [21/40] |	nca: 4.128794997930527, flat: 1.1299818381667137, pod: 12.923718094825745, loss: 18.18249523639679 
Train [5/11] | Epoch [22/40] |	nca: 2.280726984143257, flat: 1.103534109890461, pod: 12.274252951145172, loss: 15.658514142036438 
Train [5/11] | Epoch [23/40] |	nca: 1.9287112951278687, flat: 1.06968442350626, pod: 12.195978462696075, loss: 15.194374203681946 
Train [5/11] | Epoch [24/40] |	nca: 1.816162347793579, flat: 1.0998660549521446, pod: 12.351706683635712, loss: 15.26773488521576 
Train [5/11] | Epoch [25/40] |	nca: 1.8513469025492668, flat: 1.0900304913520813, pod: 12.403236627578735, loss: 15.344614148139954 
Train [5/11] | Epoch [26/40] |	nca: 1.5916002839803696, flat: 1.0731444135308266, pod: 12.229283571243286, loss: 14.894028186798096 
Train [5/11] | Epoch [27/40] |	nca: 1.6850028485059738, flat: 1.1185335144400597, pod: 12.462763726711273, loss: 15.266300082206726 
Train [5/11] | Epoch [28/40] |	nca: 1.501622699201107, flat: 1.1082365438342094, pod: 12.281569421291351, loss: 14.891428709030151 
Train [5/11] | Epoch [29/40] |	nca: 1.5069767013192177, flat: 1.1036743000149727, pod: 12.52151483297348, loss: 15.132166028022766 
Train [5/11] | Epoch [30/40] |	nca: 1.4810432568192482, flat: 1.1016928404569626, pod: 12.510382235050201, loss: 15.09311830997467 
Train [5/11] | Epoch [31/40] |	nca: 1.5158952623605728, flat: 1.1045426651835442, pod: 12.502391934394836, loss: 15.122829914093018 
Train [5/11] | Epoch [32/40] |	nca: 1.496378168463707, flat: 1.0694059506058693, pod: 12.027833878993988, loss: 14.593618273735046 
Train [5/11] | Epoch [33/40] |	nca: 1.3918403685092926, flat: 1.0988352671265602, pod: 12.354240894317627, loss: 14.844916343688965 
Train [5/11] | Epoch [34/40] |	nca: 1.333365075290203, flat: 1.1050894632935524, pod: 12.604431867599487, loss: 15.04288649559021 
Train [5/11] | Epoch [35/40] |	nca: 1.359220065176487, flat: 1.1096913293004036, pod: 12.580783426761627, loss: 15.049694776535034 
Train [5/11] | Epoch [36/40] |	nca: 1.3203308954834938, flat: 1.113683506846428, pod: 12.487567663192749, loss: 14.921581983566284 
Train [5/11] | Epoch [37/40] |	nca: 1.295709490776062, flat: 1.0978062450885773, pod: 12.283936738967896, loss: 14.677452325820923 
Train [5/11] | Epoch [38/40] |	nca: 1.2930617183446884, flat: 1.0954196229577065, pod: 12.303721487522125, loss: 14.692202687263489 
Train [5/11] | Epoch [39/40] |	nca: 1.380323812365532, flat: 1.0819116681814194, pod: 12.335566580295563, loss: 14.797801971435547 
Train [5/11] | Epoch [40/40] |	nca: 1.3456824719905853, flat: 1.0804614275693893, pod: 12.413159012794495, loss: 14.839303016662598 
after task
Building & updating memory.
after task
Eval on 0->70.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.6192.
Current acc: {'total': 0.558, '00-09': 0.588, '10-19': 0.523, '20-29': 0.493, '30-39': 0.496, '40-49': 0.582, '50-59': 0.608, '60-69': 0.619}.
Avg inc acc top5: 0.8775999999999999.
Current acc top5: {'total': 0.832}.
Forgetting: 0.111125.
Cord metric: 0.62.
Old accuracy: 0.55, mean: 0.59.
New accuracy: 0.68, mean: 0.69.
================Task 5 Start!================
Testing on False unseen tasks (max class = 75).
Set memory of size: 1400.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 5 Training!================
The training samples number: 3900
Train on 70->75.
train task
nb 3900.
Train [6/11] | Epoch [1/20] |	nca: 20.84548956155777, flat: 3.835455894470215, pod: 47.700572311878204, loss: 72.38151800632477 
Train [6/11] | Epoch [2/20] |	nca: 15.379753291606903, flat: 4.316040493547916, pod: 54.02089548110962, loss: 73.71668934822083 
Train [6/11] | Epoch [3/20] |	nca: 13.032981544733047, flat: 3.823282189667225, pod: 50.78683316707611, loss: 67.64309632778168 
Train [6/11] | Epoch [4/20] |	nca: 11.419654041528702, flat: 3.429238423705101, pod: 48.15954339504242, loss: 63.0084365606308 
Train [6/11] | Epoch [5/20] |	nca: 11.482088595628738, flat: 3.295171305537224, pod: 46.86496460437775, loss: 61.642224073410034 
Train [6/11] | Epoch [6/20] |	nca: 10.867887660861015, flat: 3.5887879133224487, pod: 48.38491678237915, loss: 62.84159255027771 
Train [6/11] | Epoch [7/20] |	nca: 8.955104768276215, flat: 3.04694015532732, pod: 43.05181574821472, loss: 55.053860545158386 
Train [6/11] | Epoch [8/20] |	nca: 8.00763463973999, flat: 2.6729315146803856, pod: 41.1346800327301, loss: 51.81524610519409 
Train [6/11] | Epoch [9/20] |	nca: 8.987513706088066, flat: 2.8623699694871902, pod: 44.04311800003052, loss: 55.893001556396484 
Train [6/11] | Epoch [10/20] |	nca: 7.649783372879028, flat: 2.367732010781765, pod: 37.9384571313858, loss: 47.95597243309021 
Train [6/11] | Epoch [11/20] |	nca: 7.405522048473358, flat: 2.3603020384907722, pod: 38.05246305465698, loss: 47.81828689575195 
Train [6/11] | Epoch [12/20] |	nca: 6.202670156955719, flat: 2.064326211810112, pod: 35.43354481458664, loss: 43.700541257858276 
Train [6/11] | Epoch [13/20] |	nca: 6.511806130409241, flat: 1.9305552579462528, pod: 34.258311331272125, loss: 42.70067274570465 
Train [6/11] | Epoch [14/20] |	nca: 5.9043979793787, flat: 1.8221071511507034, pod: 33.47466039657593, loss: 41.20116567611694 
Train [6/11] | Epoch [15/20] |	nca: 5.605104580521584, flat: 1.5916378051042557, pod: 29.988270342350006, loss: 37.18501257896423 
Train [6/11] | Epoch [16/20] |	nca: 5.712654858827591, flat: 1.4728291183710098, pod: 27.947439312934875, loss: 35.132922887802124 
Train [6/11] | Epoch [17/20] |	nca: 5.374265305697918, flat: 1.4138918705284595, pod: 27.69268536567688, loss: 34.48084259033203 
Train [6/11] | Epoch [18/20] |	nca: 5.108588881790638, flat: 1.3265188112854958, pod: 26.188723504543304, loss: 32.62383097410202 
Train [6/11] | Epoch [19/20] |	nca: 5.153371758759022, flat: 1.2803592681884766, pod: 25.020646035671234, loss: 31.45437717437744 
Train [6/11] | Epoch [20/20] |	nca: 4.890018984675407, flat: 1.1746647488325834, pod: 23.523948967456818, loss: 29.588632702827454 
Fine-tuning
Building & updating memory.
Train [6/11] | Epoch [21/40] |	nca: 3.102670669555664, flat: 0.9399712570011616, pod: 13.04237186908722, loss: 17.08501398563385 
Train [6/11] | Epoch [22/40] |	nca: 1.7370026409626007, flat: 0.9480953365564346, pod: 13.074874222278595, loss: 15.759972214698792 
Train [6/11] | Epoch [23/40] |	nca: 1.7512604743242264, flat: 0.9440807327628136, pod: 12.96493250131607, loss: 15.660273671150208 
Train [6/11] | Epoch [24/40] |	nca: 1.4610929787158966, flat: 0.9403301738202572, pod: 12.898155510425568, loss: 15.299578666687012 
Train [6/11] | Epoch [25/40] |	nca: 1.3706298395991325, flat: 0.9369625598192215, pod: 12.895000874996185, loss: 15.202593326568604 
Train [6/11] | Epoch [26/40] |	nca: 1.424138404428959, flat: 0.9517676755785942, pod: 13.117680311203003, loss: 15.4935861825943 
Train [6/11] | Epoch [27/40] |	nca: 1.348541907966137, flat: 0.9349207691848278, pod: 12.962415158748627, loss: 15.245877504348755 
Train [6/11] | Epoch [28/40] |	nca: 1.3363846093416214, flat: 0.9362907037138939, pod: 12.614860713481903, loss: 14.88753604888916 
Train [6/11] | Epoch [29/40] |	nca: 1.3066474795341492, flat: 0.9574933722615242, pod: 13.086673080921173, loss: 15.35081398487091 
Train [6/11] | Epoch [30/40] |	nca: 1.263202078640461, flat: 0.9400546252727509, pod: 13.12974989414215, loss: 15.333006501197815 
Train [6/11] | Epoch [31/40] |	nca: 1.2642189860343933, flat: 0.9766431823372841, pod: 13.484223127365112, loss: 15.725085377693176 
Train [6/11] | Epoch [32/40] |	nca: 1.3125573694705963, flat: 0.978697270154953, pod: 13.599346339702606, loss: 15.8906010389328 
Train [6/11] | Epoch [33/40] |	nca: 1.1858804821968079, flat: 0.9303104728460312, pod: 12.700495302677155, loss: 14.816686272621155 
Train [6/11] | Epoch [34/40] |	nca: 1.2507223188877106, flat: 0.9414145834743977, pod: 13.107796430587769, loss: 15.299933314323425 
Train [6/11] | Epoch [35/40] |	nca: 1.1515186503529549, flat: 0.9370269179344177, pod: 12.92083466053009, loss: 15.009380102157593 
Train [6/11] | Epoch [36/40] |	nca: 1.1996405273675919, flat: 0.9207720458507538, pod: 12.660945177078247, loss: 14.781357884407043 
Train [6/11] | Epoch [37/40] |	nca: 1.1026825606822968, flat: 0.9253750666975975, pod: 13.047121465206146, loss: 15.075179100036621 
Train [6/11] | Epoch [38/40] |	nca: 1.153310663998127, flat: 0.9273594431579113, pod: 12.9246746301651, loss: 15.005344808101654 
Train [6/11] | Epoch [39/40] |	nca: 1.1415525898337364, flat: 0.9492750018835068, pod: 13.000919163227081, loss: 15.09174656867981 
Train [6/11] | Epoch [40/40] |	nca: 1.0664509758353233, flat: 0.9438084363937378, pod: 12.938098907470703, loss: 14.948358535766602 
after task
Building & updating memory.
after task
Eval on 0->75.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.6076666666666667.
Current acc: {'total': 0.55, '00-09': 0.554, '10-19': 0.489, '20-29': 0.487, '30-39': 0.49, '40-49': 0.554, '50-59': 0.58, '60-69': 0.581, '70-79': 0.786}.
Avg inc acc top5: 0.8694999999999999.
Current acc top5: {'total': 0.829}.
Forgetting: 0.03555555555555556.
Cord metric: 0.61.
Old accuracy: 0.53, mean: 0.58.
New accuracy: 0.79, mean: 0.71.
================Task 6 Start!================
Testing on False unseen tasks (max class = 80).
Set memory of size: 1500.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 6 Training!================
The training samples number: 4000
Train on 75->80.
train task
nb 4000.
Train [7/11] | Epoch [1/20] |	nca: 22.955356746912003, flat: 3.8758127614855766, pod: 48.04339426755905, loss: 74.8745630979538 
Train [7/11] | Epoch [2/20] |	nca: 17.46750608086586, flat: 4.170385196805, pod: 53.01874363422394, loss: 74.65663504600525 
Train [7/11] | Epoch [3/20] |	nca: 15.880302399396896, flat: 3.8992995768785477, pod: 52.974892377853394, loss: 72.75449466705322 
Train [7/11] | Epoch [4/20] |	nca: 13.734505444765091, flat: 3.5048855990171432, pod: 49.47030973434448, loss: 66.70970058441162 
Train [7/11] | Epoch [5/20] |	nca: 12.706175178289413, flat: 3.2846506386995316, pod: 48.08581364154816, loss: 64.07663905620575 
Train [7/11] | Epoch [6/20] |	nca: 12.17728540301323, flat: 3.1829183027148247, pod: 46.92002272605896, loss: 62.28022634983063 
Train [7/11] | Epoch [7/20] |	nca: 12.359254777431488, flat: 3.315369226038456, pod: 46.53085732460022, loss: 62.20548164844513 
Train [7/11] | Epoch [8/20] |	nca: 10.999311998486519, flat: 2.925887167453766, pod: 44.11104071140289, loss: 58.036240220069885 
Train [7/11] | Epoch [9/20] |	nca: 9.970161318778992, flat: 2.7350318282842636, pod: 42.66787540912628, loss: 55.3730685710907 
Train [7/11] | Epoch [10/20] |	nca: 9.224059641361237, flat: 2.4197688922286034, pod: 40.05573236942291, loss: 51.69956111907959 
Train [7/11] | Epoch [11/20] |	nca: 8.69730195403099, flat: 2.3610173873603344, pod: 39.864035844802856, loss: 50.9223552942276 
Train [7/11] | Epoch [12/20] |	nca: 8.60719819366932, flat: 2.207035433501005, pod: 37.216347455978394, loss: 48.03058111667633 
Train [7/11] | Epoch [13/20] |	nca: 8.293858543038368, flat: 2.1094021648168564, pod: 36.874793350696564, loss: 47.27805435657501 
Train [7/11] | Epoch [14/20] |	nca: 7.620085373520851, flat: 2.0505208522081375, pod: 35.16833817958832, loss: 44.83894467353821 
Train [7/11] | Epoch [15/20] |	nca: 6.836035892367363, flat: 1.6620604656636715, pod: 32.14728116989136, loss: 40.64537763595581 
Train [7/11] | Epoch [16/20] |	nca: 6.693004071712494, flat: 1.5224671252071857, pod: 30.002144932746887, loss: 38.21761643886566 
Train [7/11] | Epoch [17/20] |	nca: 7.081596240401268, flat: 1.4388229735195637, pod: 28.060616493225098, loss: 36.58103555440903 
Train [7/11] | Epoch [18/20] |	nca: 6.5855085253715515, flat: 1.3955409936606884, pod: 26.941407203674316, loss: 34.92245692014694 
Train [7/11] | Epoch [19/20] |	nca: 6.44503390789032, flat: 1.239423057064414, pod: 25.410902857780457, loss: 33.095359802246094 
Train [7/11] | Epoch [20/20] |	nca: 6.159193255007267, flat: 1.2126854546368122, pod: 24.457516610622406, loss: 31.829395413398743 
Fine-tuning
Building & updating memory.
Train [7/11] | Epoch [21/40] |	nca: 3.519320636987686, flat: 0.8647022657096386, pod: 14.889645636081696, loss: 19.27366876602173 
Train [7/11] | Epoch [22/40] |	nca: 2.107477881014347, flat: 0.8646167367696762, pod: 14.743298530578613, loss: 17.71539318561554 
Train [7/11] | Epoch [23/40] |	nca: 1.770958051085472, flat: 0.8802164494991302, pod: 14.971682012081146, loss: 17.622856497764587 
Train [7/11] | Epoch [24/40] |	nca: 1.634277604520321, flat: 0.8657179810106754, pod: 14.873129904270172, loss: 17.373125314712524 
Train [7/11] | Epoch [25/40] |	nca: 1.599203772842884, flat: 0.8947690054774284, pod: 15.26645290851593, loss: 17.760425806045532 
Train [7/11] | Epoch [26/40] |	nca: 1.608084686100483, flat: 0.8455378636717796, pod: 14.756796419620514, loss: 17.210419058799744 
Train [7/11] | Epoch [27/40] |	nca: 1.5725651234388351, flat: 0.8743825145065784, pod: 14.831881701946259, loss: 17.278829216957092 
Train [7/11] | Epoch [28/40] |	nca: 1.5355329066514969, flat: 0.8796085268259048, pod: 14.89800214767456, loss: 17.313143610954285 
Train [7/11] | Epoch [29/40] |	nca: 1.416054368019104, flat: 0.8559263721108437, pod: 14.832812666893005, loss: 17.10479348897934 
Train [7/11] | Epoch [30/40] |	nca: 1.4075968191027641, flat: 0.8715601712465286, pod: 15.12160861492157, loss: 17.400765895843506 
Train [7/11] | Epoch [31/40] |	nca: 1.4214576408267021, flat: 0.8664763048291206, pod: 14.857450604438782, loss: 17.145384311676025 
Train [7/11] | Epoch [32/40] |	nca: 1.3446886017918587, flat: 0.8374610245227814, pod: 14.519541919231415, loss: 16.701691389083862 
Train [7/11] | Epoch [33/40] |	nca: 1.3855081498622894, flat: 0.8989942595362663, pod: 15.044479548931122, loss: 17.32898199558258 
Train [7/11] | Epoch [34/40] |	nca: 1.3435890823602676, flat: 0.89298490062356, pod: 15.181227326393127, loss: 17.41780149936676 
Train [7/11] | Epoch [35/40] |	nca: 1.3023600205779076, flat: 0.8593409061431885, pod: 14.663566589355469, loss: 16.82526731491089 
Train [7/11] | Epoch [36/40] |	nca: 1.2936419919133186, flat: 0.8850913271307945, pod: 14.956818580627441, loss: 17.135551810264587 
Train [7/11] | Epoch [37/40] |	nca: 1.4239027798175812, flat: 0.8517235219478607, pod: 14.633431494235992, loss: 16.90905785560608 
Train [7/11] | Epoch [38/40] |	nca: 1.2921651005744934, flat: 0.8903517089784145, pod: 15.28246295452118, loss: 17.464979767799377 
Train [7/11] | Epoch [39/40] |	nca: 1.2121843174099922, flat: 0.8712572231888771, pod: 14.85089361667633, loss: 16.93433529138565 
Train [7/11] | Epoch [40/40] |	nca: 1.2450148910284042, flat: 0.8447795026004314, pod: 14.662692666053772, loss: 16.752487182617188 
after task
Building & updating memory.
after task
Eval on 0->80.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.5970000000000001.
Current acc: {'total': 0.533, '00-09': 0.546, '10-19': 0.472, '20-29': 0.441, '30-39': 0.476, '40-49': 0.532, '50-59': 0.569, '60-69': 0.523, '70-79': 0.708}.
Avg inc acc top5: 0.8611428571428571.
Current acc top5: {'total': 0.811}.
Forgetting: 0.1511111111111111.
Cord metric: 0.60.
Old accuracy: 0.52, mean: 0.57.
New accuracy: 0.68, mean: 0.71.
================Task 7 Start!================
Testing on False unseen tasks (max class = 85).
Set memory of size: 1600.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 7 Training!================
The training samples number: 4100
Train on 80->85.
train task
nb 4100.
Train [8/11] | Epoch [1/20] |	nca: 24.419847518205643, flat: 4.6119419150054455, pod: 54.715219378471375, loss: 83.74700891971588 
Train [8/11] | Epoch [2/20] |	nca: 35.17623829841614, flat: 12.21540743112564, pod: 82.54288721084595, loss: 129.9345338344574 
Train [8/11] | Epoch [3/20] |	nca: 26.244782477617264, flat: 10.255132004618645, pod: 81.4044554233551, loss: 117.90437126159668 
Train [8/11] | Epoch [4/20] |	nca: 33.5018310546875, flat: 12.492194071412086, pod: 98.22281694412231, loss: 144.21684098243713 
Train [8/11] | Epoch [5/20] |	nca: 24.533365160226822, flat: 9.327030673623085, pod: 81.23946976661682, loss: 115.09986639022827 
Train [8/11] | Epoch [6/20] |	nca: 23.692977637052536, flat: 9.6339972615242, pod: 83.49256145954132, loss: 116.81953620910645 
Train [8/11] | Epoch [7/20] |	nca: 21.122960172593594, flat: 8.317327320575714, pod: 76.33920156955719, loss: 105.77948927879333 
Train [8/11] | Epoch [8/20] |	nca: 14.104853689670563, flat: 5.3756359070539474, pod: 60.182430267333984, loss: 79.66291999816895 
Train [8/11] | Epoch [9/20] |	nca: 16.159487277269363, flat: 6.017406314611435, pod: 63.652597308158875, loss: 85.82949113845825 
Train [8/11] | Epoch [10/20] |	nca: 15.822746753692627, flat: 5.562395326793194, pod: 60.685099959373474, loss: 82.07024216651917 
Train [8/11] | Epoch [11/20] |	nca: 15.813381224870682, flat: 5.689375229179859, pod: 57.717602610588074, loss: 79.2203596830368 
Train [8/11] | Epoch [12/20] |	nca: 15.833265960216522, flat: 6.082652546465397, pod: 59.148980259895325, loss: 81.06489837169647 
Train [8/11] | Epoch [13/20] |	nca: 13.417095199227333, flat: 4.915004290640354, pod: 55.49023628234863, loss: 73.82233679294586 
Train [8/11] | Epoch [14/20] |	nca: 11.154196977615356, flat: 3.6154690012335777, pod: 46.14447581768036, loss: 60.91414189338684 
Train [8/11] | Epoch [15/20] |	nca: 11.233824506402016, flat: 3.6350481510162354, pod: 45.52680480480194, loss: 60.39567732810974 
Train [8/11] | Epoch [16/20] |	nca: 9.47696390748024, flat: 3.4099280387163162, pod: 42.73289954662323, loss: 55.61979138851166 
Train [8/11] | Epoch [17/20] |	nca: 8.387993320822716, flat: 2.816125527024269, pod: 39.44950580596924, loss: 50.65362477302551 
Train [8/11] | Epoch [18/20] |	nca: 8.35031408071518, flat: 2.7398620918393135, pod: 38.00475376844406, loss: 49.09493005275726 
Train [8/11] | Epoch [19/20] |	nca: 10.073820039629936, flat: 2.9110129326581955, pod: 37.46922332048416, loss: 50.45405662059784 
Train [8/11] | Epoch [20/20] |	nca: 9.614364132285118, flat: 2.706900630146265, pod: 36.8460698723793, loss: 49.16733419895172 
Fine-tuning
Building & updating memory.
Train [8/11] | Epoch [21/40] |	nca: 5.178470849990845, flat: 1.4735366255044937, pod: 20.671282410621643, loss: 27.323289275169373 
Train [8/11] | Epoch [22/40] |	nca: 3.3818663209676743, flat: 1.5344861522316933, pod: 21.059199333190918, loss: 25.97555160522461 
Train [8/11] | Epoch [23/40] |	nca: 3.059301659464836, flat: 1.5134302005171776, pod: 21.100046634674072, loss: 25.672778367996216 
Train [8/11] | Epoch [24/40] |	nca: 2.7606462091207504, flat: 1.5607446283102036, pod: 21.21098780632019, loss: 25.532378554344177 
Train [8/11] | Epoch [25/40] |	nca: 2.65546153485775, flat: 1.4806176722049713, pod: 20.766355633735657, loss: 24.902434706687927 
Train [8/11] | Epoch [26/40] |	nca: 2.5857065618038177, flat: 1.4519381821155548, pod: 20.58704149723053, loss: 24.624686360359192 
Train [8/11] | Epoch [27/40] |	nca: 2.5933635979890823, flat: 1.4376356601715088, pod: 20.64409041404724, loss: 24.675089716911316 
Train [8/11] | Epoch [28/40] |	nca: 2.493448629975319, flat: 1.4612007439136505, pod: 20.799299955368042, loss: 24.753949403762817 
Train [8/11] | Epoch [29/40] |	nca: 2.3702867329120636, flat: 1.4296856224536896, pod: 20.515331387519836, loss: 24.315303564071655 
Train [8/11] | Epoch [30/40] |	nca: 2.366807997226715, flat: 1.4373801499605179, pod: 20.41179120540619, loss: 24.21597933769226 
Train [8/11] | Epoch [31/40] |	nca: 2.452237457036972, flat: 1.497861698269844, pod: 20.990864992141724, loss: 24.940964221954346 
Train [8/11] | Epoch [32/40] |	nca: 2.405358910560608, flat: 1.4857815876603127, pod: 20.951790928840637, loss: 24.842931509017944 
Train [8/11] | Epoch [33/40] |	nca: 2.1465805545449257, flat: 1.458078272640705, pod: 20.790773630142212, loss: 24.395432472229004 
Train [8/11] | Epoch [34/40] |	nca: 2.2831199169158936, flat: 1.4792865440249443, pod: 20.82369029521942, loss: 24.58609676361084 
Train [8/11] | Epoch [35/40] |	nca: 2.2903705686330795, flat: 1.4505098089575768, pod: 20.628655433654785, loss: 24.36953580379486 
Train [8/11] | Epoch [36/40] |	nca: 2.1699909418821335, flat: 1.4419438689947128, pod: 20.556612372398376, loss: 24.16854703426361 
Train [8/11] | Epoch [37/40] |	nca: 2.1840860098600388, flat: 1.4885567054152489, pod: 20.922281622886658, loss: 24.594924449920654 
Train [8/11] | Epoch [38/40] |	nca: 2.188648894429207, flat: 1.4867196902632713, pod: 20.796563744544983, loss: 24.471932291984558 
Train [8/11] | Epoch [39/40] |	nca: 2.2712880820035934, flat: 1.508943259716034, pod: 21.18653440475464, loss: 24.966765880584717 
Train [8/11] | Epoch [40/40] |	nca: 2.167059503495693, flat: 1.4693591445684433, pod: 20.77678906917572, loss: 24.413207530975342 
after task
Building & updating memory.
after task
Eval on 0->85.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.587375.
Current acc: {'total': 0.52, '00-09': 0.547, '10-19': 0.455, '20-29': 0.451, '30-39': 0.465, '40-49': 0.515, '50-59': 0.521, '60-69': 0.495, '70-79': 0.617, '80-89': 0.702}.
Avg inc acc top5: 0.8548749999999999.
Current acc top5: {'total': 0.811}.
Forgetting: 0.0859.
Cord metric: 0.59.
Old accuracy: 0.51, mean: 0.56.
New accuracy: 0.70, mean: 0.70.
================Task 8 Start!================
Testing on False unseen tasks (max class = 90).
Set memory of size: 1700.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 8 Training!================
The training samples number: 4200
Train on 85->90.
train task
nb 4200.
Train [9/11] | Epoch [1/20] |	nca: 19.72267758846283, flat: 3.171310640871525, pod: 47.11343240737915, loss: 70.00742077827454 
Train [9/11] | Epoch [2/20] |	nca: 13.329633265733719, flat: 2.4963508024811745, pod: 44.001654624938965, loss: 59.82763862609863 
Train [9/11] | Epoch [3/20] |	nca: 11.791157633066177, flat: 2.5226468704640865, pod: 46.34720289707184, loss: 60.66100716590881 
Train [9/11] | Epoch [4/20] |	nca: 11.135547012090683, flat: 2.3504980355501175, pod: 43.529523968696594, loss: 57.01556932926178 
Train [9/11] | Epoch [5/20] |	nca: 10.25646060705185, flat: 2.6686726436018944, pod: 46.01318943500519, loss: 58.9383225440979 
Train [9/11] | Epoch [6/20] |	nca: 9.825882732868195, flat: 2.4528437927365303, pod: 44.14839327335358, loss: 56.42711961269379 
Train [9/11] | Epoch [7/20] |	nca: 9.088032603263855, flat: 2.169079929590225, pod: 40.44251310825348, loss: 51.69962561130524 
Train [9/11] | Epoch [8/20] |	nca: 8.701940774917603, flat: 2.1560772135853767, pod: 41.364702343940735, loss: 52.2227201461792 
Train [9/11] | Epoch [9/20] |	nca: 8.149663165211678, flat: 2.135979227721691, pod: 41.29154109954834, loss: 51.57718372344971 
Train [9/11] | Epoch [10/20] |	nca: 8.182850539684296, flat: 2.1171515807509422, pod: 39.53822600841522, loss: 49.83822774887085 
Train [9/11] | Epoch [11/20] |	nca: 7.443825364112854, flat: 1.8257112689316273, pod: 36.97596454620361, loss: 46.24550104141235 
Train [9/11] | Epoch [12/20] |	nca: 7.383506715297699, flat: 1.6307501941919327, pod: 34.5548712015152, loss: 43.56912839412689 
Train [9/11] | Epoch [13/20] |	nca: 7.136320799589157, flat: 1.5784967355430126, pod: 33.390810906887054, loss: 42.105628490448 
Train [9/11] | Epoch [14/20] |	nca: 6.6492752730846405, flat: 1.540879163891077, pod: 33.2038654088974, loss: 41.39401996135712 
Train [9/11] | Epoch [15/20] |	nca: 6.508034139871597, flat: 1.3966669999063015, pod: 30.626530826091766, loss: 38.53123211860657 
Train [9/11] | Epoch [16/20] |	nca: 6.336402624845505, flat: 1.2635207306593657, pod: 29.051170766353607, loss: 36.65109431743622 
Train [9/11] | Epoch [17/20] |	nca: 6.185086660087109, flat: 1.1228757072240114, pod: 26.252351343631744, loss: 33.56031382083893 
Train [9/11] | Epoch [18/20] |	nca: 6.154043078422546, flat: 1.1322524268180132, pod: 27.1804558634758, loss: 34.46675157546997 
Train [9/11] | Epoch [19/20] |	nca: 5.864939816296101, flat: 1.0367580074816942, pod: 24.157255470752716, loss: 31.058953285217285 
Train [9/11] | Epoch [20/20] |	nca: 5.932457998394966, flat: 1.0086739901453257, pod: 23.426507771015167, loss: 30.36763972043991 
Fine-tuning
Building & updating memory.
Train [9/11] | Epoch [21/40] |	nca: 4.626886829733849, flat: 1.240975171327591, pod: 20.09329044818878, loss: 25.96115267276764 
Train [9/11] | Epoch [22/40] |	nca: 4.07335838675499, flat: 1.1634420230984688, pod: 19.64911949634552, loss: 24.88591992855072 
Train [9/11] | Epoch [23/40] |	nca: 3.654983952641487, flat: 1.2032489255070686, pod: 20.29438954591751, loss: 25.15262222290039 
Train [9/11] | Epoch [24/40] |	nca: 3.915747880935669, flat: 1.2255517579615116, pod: 20.017037212848663, loss: 25.158336639404297 
Train [9/11] | Epoch [25/40] |	nca: 3.4777875542640686, flat: 1.2057307995855808, pod: 20.155576288700104, loss: 24.839094758033752 
Train [9/11] | Epoch [26/40] |	nca: 3.677813872694969, flat: 1.2083353884518147, pod: 20.073039412498474, loss: 24.959188580513 
Train [9/11] | Epoch [27/40] |	nca: 4.168494567275047, flat: 1.214881107211113, pod: 19.726142525672913, loss: 25.10951817035675 
Train [9/11] | Epoch [28/40] |	nca: 3.8682427555322647, flat: 1.2354169748723507, pod: 20.116885781288147, loss: 25.220545411109924 
Train [9/11] | Epoch [29/40] |	nca: 3.814941257238388, flat: 1.3158385455608368, pod: 21.00742071866989, loss: 26.138200759887695 
Train [9/11] | Epoch [30/40] |	nca: 4.502102971076965, flat: 1.2531780265271664, pod: 20.37512731552124, loss: 26.13040816783905 
Train [9/11] | Epoch [31/40] |	nca: 4.117915496230125, flat: 1.202218446880579, pod: 20.319023311138153, loss: 25.63915753364563 
Train [9/11] | Epoch [32/40] |	nca: 4.034993469715118, flat: 1.151626005768776, pod: 19.613476395606995, loss: 24.800095796585083 
Train [9/11] | Epoch [33/40] |	nca: 3.2553724348545074, flat: 1.0900267958641052, pod: 19.72334313392639, loss: 24.068742156028748 
Train [9/11] | Epoch [34/40] |	nca: 3.271028846502304, flat: 1.1885097734630108, pod: 20.029155492782593, loss: 24.488694190979004 
Train [9/11] | Epoch [35/40] |	nca: 2.843860223889351, flat: 1.1232838034629822, pod: 19.745798707008362, loss: 23.712942719459534 
Train [9/11] | Epoch [36/40] |	nca: 2.479367256164551, flat: 1.3248569071292877, pod: 20.707571864128113, loss: 24.51179599761963 
Train [9/11] | Epoch [37/40] |	nca: 2.4679841101169586, flat: 1.245962928980589, pod: 19.937960505485535, loss: 23.651907682418823 
Train [9/11] | Epoch [38/40] |	nca: 2.432684138417244, flat: 1.1736015677452087, pod: 20.240379333496094, loss: 23.846665024757385 
Train [9/11] | Epoch [39/40] |	nca: 3.3938398510217667, flat: 1.2034556493163109, pod: 19.742767691612244, loss: 24.340062975883484 
Train [9/11] | Epoch [40/40] |	nca: 3.0977746695280075, flat: 1.167467512190342, pod: 20.243057668209076, loss: 24.508300065994263 
after task
Building & updating memory.
after task
Eval on 0->90.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.5777777777777778.
Current acc: {'total': 0.501, '00-09': 0.547, '10-19': 0.458, '20-29': 0.399, '30-39': 0.457, '40-49': 0.519, '50-59': 0.533, '60-69': 0.459, '70-79': 0.546, '80-89': 0.593}.
Avg inc acc top5: 0.848111111111111.
Current acc top5: {'total': 0.794}.
Forgetting: 0.18179999999999996.
Cord metric: 0.58.
Old accuracy: 0.49, mean: 0.55.
New accuracy: 0.62, mean: 0.69.
================Task 9 Start!================
Testing on False unseen tasks (max class = 95).
Set memory of size: 1800.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 9 Training!================
The training samples number: 4300
Train on 90->95.
train task
nb 4300.
Train [10/11] | Epoch [1/20] |	nca: 26.725886404514313, flat: 4.006527978926897, pod: 50.48699361085892, loss: 81.21940875053406 
Train [10/11] | Epoch [2/20] |	nca: 19.229735404253006, flat: 3.8931958004832268, pod: 54.568323254585266, loss: 77.69125509262085 
Train [10/11] | Epoch [3/20] |	nca: 17.253474563360214, flat: 3.580463230609894, pod: 50.43512427806854, loss: 71.26906192302704 
Train [10/11] | Epoch [4/20] |	nca: 14.9726542532444, flat: 3.4741645455360413, pod: 49.6248722076416, loss: 68.07169091701508 
Train [10/11] | Epoch [5/20] |	nca: 14.094876080751419, flat: 3.518471822142601, pod: 49.8131867647171, loss: 67.42653524875641 
Train [10/11] | Epoch [6/20] |	nca: 13.545845746994019, flat: 3.415630526840687, pod: 49.36746788024902, loss: 66.32894468307495 
Train [10/11] | Epoch [7/20] |	nca: 12.278238534927368, flat: 3.2036199271678925, pod: 47.75727999210358, loss: 63.23913872241974 
Train [10/11] | Epoch [8/20] |	nca: 11.496504783630371, flat: 2.9590982124209404, pod: 46.02071666717529, loss: 60.476319670677185 
Train [10/11] | Epoch [9/20] |	nca: 11.44424606859684, flat: 2.873929686844349, pod: 43.80489540100098, loss: 58.12307095527649 
Train [10/11] | Epoch [10/20] |	nca: 9.76919810473919, flat: 2.513873126357794, pod: 41.007139682769775, loss: 53.29021084308624 
Train [10/11] | Epoch [11/20] |	nca: 10.091201663017273, flat: 2.4249873980879784, pod: 40.69654023647308, loss: 53.21272873878479 
Train [10/11] | Epoch [12/20] |	nca: 9.20528893172741, flat: 2.1744504123926163, pod: 37.61562532186508, loss: 48.99536418914795 
Train [10/11] | Epoch [13/20] |	nca: 8.603116378188133, flat: 1.997923407703638, pod: 35.0512530207634, loss: 45.652292251586914 
Train [10/11] | Epoch [14/20] |	nca: 8.486900269985199, flat: 1.8915622495114803, pod: 34.47864133119583, loss: 44.85710346698761 
Train [10/11] | Epoch [15/20] |	nca: 8.318778082728386, flat: 1.6659575626254082, pod: 31.451777815818787, loss: 41.4365131855011 
Train [10/11] | Epoch [16/20] |	nca: 7.754509702324867, flat: 1.7016829699277878, pod: 31.129073798656464, loss: 40.58526635169983 
Train [10/11] | Epoch [17/20] |	nca: 7.886008948087692, flat: 1.63176254555583, pod: 30.346822023391724, loss: 39.86459368467331 
Train [10/11] | Epoch [18/20] |	nca: 7.347900673747063, flat: 1.4360809475183487, pod: 27.531540870666504, loss: 36.31552267074585 
Train [10/11] | Epoch [19/20] |	nca: 7.209742218255997, flat: 1.3974815122783184, pod: 27.0711070895195, loss: 35.67833077907562 
Train [10/11] | Epoch [20/20] |	nca: 6.87875297665596, flat: 1.3330762293189764, pod: 25.92483127117157, loss: 34.1366605758667 
Fine-tuning
Building & updating memory.
Train [10/11] | Epoch [21/40] |	nca: 3.532424882054329, flat: 0.7462591864168644, pod: 13.174257099628448, loss: 17.452941238880157 
Train [10/11] | Epoch [22/40] |	nca: 2.369910329580307, flat: 0.7386881224811077, pod: 13.325361728668213, loss: 16.43396019935608 
Train [10/11] | Epoch [23/40] |	nca: 2.233695276081562, flat: 0.7566787824034691, pod: 13.423088014125824, loss: 16.413462162017822 
Train [10/11] | Epoch [24/40] |	nca: 1.9942457228899002, flat: 0.7541020326316357, pod: 13.79326981306076, loss: 16.541617512702942 
Train [10/11] | Epoch [25/40] |	nca: 1.929358296096325, flat: 0.7531995810568333, pod: 13.193511009216309, loss: 15.876068890094757 
Train [10/11] | Epoch [26/40] |	nca: 1.8054975643754005, flat: 0.7647655159235001, pod: 13.532240390777588, loss: 16.102503657341003 
Train [10/11] | Epoch [27/40] |	nca: 1.78510432690382, flat: 0.7663407605141401, pod: 13.644432961940765, loss: 16.195877969264984 
Train [10/11] | Epoch [28/40] |	nca: 1.7421669214963913, flat: 0.7514483593404293, pod: 13.044515907764435, loss: 15.53813099861145 
Train [10/11] | Epoch [29/40] |	nca: 1.6044268682599068, flat: 0.7467399761080742, pod: 13.033935964107513, loss: 15.38510274887085 
Train [10/11] | Epoch [30/40] |	nca: 1.7123826369643211, flat: 0.7785969823598862, pod: 13.359778702259064, loss: 15.85075855255127 
Train [10/11] | Epoch [31/40] |	nca: 1.6651206091046333, flat: 0.7667136341333389, pod: 13.264738738536835, loss: 15.696572840213776 
Train [10/11] | Epoch [32/40] |	nca: 1.6813545525074005, flat: 0.7916607335209846, pod: 13.913807094097137, loss: 16.38682246208191 
Train [10/11] | Epoch [33/40] |	nca: 1.581298552453518, flat: 0.7486062496900558, pod: 13.600819051265717, loss: 15.930723905563354 
Train [10/11] | Epoch [34/40] |	nca: 1.5342671275138855, flat: 0.7311545237898827, pod: 13.09558230638504, loss: 15.361004114151001 
Train [10/11] | Epoch [35/40] |	nca: 1.6113028675317764, flat: 0.7330897897481918, pod: 12.878802716732025, loss: 15.223195254802704 
Train [10/11] | Epoch [36/40] |	nca: 1.482466459274292, flat: 0.7523979619145393, pod: 13.194868803024292, loss: 15.429733157157898 
Train [10/11] | Epoch [37/40] |	nca: 1.4796190410852432, flat: 0.7325409688055515, pod: 13.147501647472382, loss: 15.35966157913208 
Train [10/11] | Epoch [38/40] |	nca: 1.5416861549019814, flat: 0.7326563559472561, pod: 13.196058094501495, loss: 15.47040057182312 
Train [10/11] | Epoch [39/40] |	nca: 1.5216622352600098, flat: 0.7518583536148071, pod: 13.395389676094055, loss: 15.668910324573517 
Train [10/11] | Epoch [40/40] |	nca: 1.4667026698589325, flat: 0.7726214453577995, pod: 13.317594587802887, loss: 15.556918799877167 
after task
Building & updating memory.
after task
Eval on 0->95.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.5694.
Current acc: {'total': 0.494, '00-09': 0.525, '10-19': 0.446, '20-29': 0.392, '30-39': 0.429, '40-49': 0.443, '50-59': 0.494, '60-69': 0.447, '70-79': 0.549, '80-89': 0.62, '90-99': 0.69}.
Avg inc acc top5: 0.8421999999999998.
Current acc top5: {'total': 0.789}.
Forgetting: 0.1176363636363636.
Cord metric: 0.57.
Old accuracy: 0.48, mean: 0.55.
New accuracy: 0.69, mean: 0.69.
================Task 10 Start!================
Testing on False unseen tasks (max class = 100).
Set memory of size: 1900.
Before task
Generating imprinted weights
Multi class diff kmeans.
Now 20 examplars per class.
Group: convnet, lr: 0.1.
Group: postprocessing, lr: 0.1.
Group: new_weights, lr: 0.1.
================Task 10 Training!================
The training samples number: 4400
Train on 95->100.
train task
nb 4400.
Train [11/11] | Epoch [1/20] |	nca: 28.453862726688385, flat: 4.534836791455746, pod: 54.29139655828476, loss: 87.28009676933289 
Train [11/11] | Epoch [2/20] |	nca: 21.35441780090332, flat: 4.787344306707382, pod: 58.425344347953796, loss: 84.56710696220398 
Train [11/11] | Epoch [3/20] |	nca: 18.18170142173767, flat: 4.192672811448574, pod: 55.37574923038483, loss: 77.7501232624054 
Train [11/11] | Epoch [4/20] |	nca: 16.966577410697937, flat: 4.283643774688244, pod: 55.93230748176575, loss: 77.18252766132355 
Train [11/11] | Epoch [5/20] |	nca: 16.117252945899963, flat: 4.22281963378191, pod: 55.38557779788971, loss: 75.72565042972565 
Train [11/11] | Epoch [6/20] |	nca: 14.87684190273285, flat: 3.829007141292095, pod: 52.70385956764221, loss: 71.4097089767456 
Train [11/11] | Epoch [7/20] |	nca: 13.720605671405792, flat: 3.620396077632904, pod: 52.112428069114685, loss: 69.45343017578125 
Train [11/11] | Epoch [8/20] |	nca: 12.107230588793755, flat: 3.169990435242653, pod: 47.523876905441284, loss: 62.80109763145447 
Train [11/11] | Epoch [9/20] |	nca: 12.048442468047142, flat: 3.1287587881088257, pod: 47.57490026950836, loss: 62.75210154056549 
Train [11/11] | Epoch [10/20] |	nca: 10.788949951529503, flat: 2.769679971039295, pod: 44.787142515182495, loss: 58.34577262401581 
Train [11/11] | Epoch [11/20] |	nca: 10.280336484313011, flat: 2.2801336348056793, pod: 39.574749410152435, loss: 52.1352196931839 
Train [11/11] | Epoch [12/20] |	nca: 9.632014378905296, flat: 2.229074899107218, pod: 39.704976975917816, loss: 51.56606650352478 
Train [11/11] | Epoch [13/20] |	nca: 9.640597462654114, flat: 2.0635999590158463, pod: 38.14515769481659, loss: 49.84935474395752 
Train [11/11] | Epoch [14/20] |	nca: 9.098368436098099, flat: 1.9840128347277641, pod: 37.062840700149536, loss: 48.145222306251526 
Train [11/11] | Epoch [15/20] |	nca: 8.178809747099876, flat: 1.695842683315277, pod: 32.36698508262634, loss: 42.2416375875473 
Train [11/11] | Epoch [16/20] |	nca: 8.100085467100143, flat: 1.6174435168504715, pod: 31.539304673671722, loss: 41.25683391094208 
Train [11/11] | Epoch [17/20] |	nca: 8.091298341751099, flat: 1.4701180458068848, pod: 30.25840735435486, loss: 39.81982362270355 
Train [11/11] | Epoch [18/20] |	nca: 7.8187991082668304, flat: 1.5149275958538055, pod: 29.57602572441101, loss: 38.90975260734558 
Train [11/11] | Epoch [19/20] |	nca: 7.596876159310341, flat: 1.3989924602210522, pod: 28.46086823940277, loss: 37.45673704147339 
Train [11/11] | Epoch [20/20] |	nca: 7.400335878133774, flat: 1.3531342539936304, pod: 27.96761691570282, loss: 36.72108727693558 
Fine-tuning
Building & updating memory.
Train [11/11] | Epoch [21/40] |	nca: 3.6359979659318924, flat: 0.8886370956897736, pod: 15.773210346698761, loss: 20.297845482826233 
Train [11/11] | Epoch [22/40] |	nca: 2.310585096478462, flat: 0.8665505759418011, pod: 15.746486783027649, loss: 18.92362254858017 
Train [11/11] | Epoch [23/40] |	nca: 2.1074620708823204, flat: 0.8376236110925674, pod: 14.972216546535492, loss: 17.91730260848999 
Train [11/11] | Epoch [24/40] |	nca: 2.0051067769527435, flat: 0.8480083607137203, pod: 15.559616923332214, loss: 18.412732124328613 
Train [11/11] | Epoch [25/40] |	nca: 1.9636425077915192, flat: 0.8576042205095291, pod: 15.5307137966156, loss: 18.3519606590271 
Train [11/11] | Epoch [26/40] |	nca: 1.9951367005705833, flat: 0.8255131058394909, pod: 15.076160848140717, loss: 17.89681088924408 
Train [11/11] | Epoch [27/40] |	nca: 1.829548455774784, flat: 0.8618986159563065, pod: 15.54971170425415, loss: 18.241158843040466 
Train [11/11] | Epoch [28/40] |	nca: 1.9172225445508957, flat: 0.8722386956214905, pod: 15.442836165428162, loss: 18.232297241687775 
Train [11/11] | Epoch [29/40] |	nca: 1.7934305742383003, flat: 0.8477488905191422, pod: 15.314854443073273, loss: 17.956034004688263 
Train [11/11] | Epoch [30/40] |	nca: 1.8902921080589294, flat: 0.8572958521544933, pod: 15.553077816963196, loss: 18.30066555738449 
Train [11/11] | Epoch [31/40] |	nca: 1.8177463933825493, flat: 0.8603014163672924, pod: 15.453065395355225, loss: 18.13111299276352 
Train [11/11] | Epoch [32/40] |	nca: 1.6775401085615158, flat: 0.8342844024300575, pod: 15.433850228786469, loss: 17.94567459821701 
Train [11/11] | Epoch [33/40] |	nca: 1.7311683222651482, flat: 0.8700615875422955, pod: 15.592931568622589, loss: 18.194161593914032 
Train [11/11] | Epoch [34/40] |	nca: 1.7405559867620468, flat: 0.8661659136414528, pod: 15.735208630561829, loss: 18.341930627822876 
Train [11/11] | Epoch [35/40] |	nca: 1.5930116400122643, flat: 0.8743356093764305, pod: 15.597882866859436, loss: 18.065230131149292 
Train [11/11] | Epoch [36/40] |	nca: 1.588421255350113, flat: 0.8819658160209656, pod: 15.630628943443298, loss: 18.101015865802765 
Train [11/11] | Epoch [37/40] |	nca: 1.5730866715312004, flat: 0.8679354004561901, pod: 15.46668267250061, loss: 17.907704830169678 
Train [11/11] | Epoch [38/40] |	nca: 1.5329583808779716, flat: 0.8671361356973648, pod: 15.649959564208984, loss: 18.050054132938385 
Train [11/11] | Epoch [39/40] |	nca: 1.5614063143730164, flat: 0.8701233007013798, pod: 15.675105512142181, loss: 18.106635212898254 
Train [11/11] | Epoch [40/40] |	nca: 1.4526583924889565, flat: 0.8622148893773556, pod: 15.310734748840332, loss: 17.625608026981354 
after task
Building & updating memory.
after task
Saving model at results\dev\podnet\202401\week_1\20240101_podnet_cnn_cifar100_50steps\net_0_task_10.pth.
Saving metadata at results\dev\podnet\202401\week_1\20240101_podnet_cnn_cifar100_50steps\meta_0_task_10.pkl.
Eval on 0->100.
eval task
podnet_cnn_cifar100_50steps
Avg inc acc: 0.5603636363636363.
Current acc: {'total': 0.47, '00-09': 0.502, '10-19': 0.412, '20-29': 0.366, '30-39': 0.415, '40-49': 0.459, '50-59': 0.474, '60-69': 0.409, '70-79': 0.516, '80-89': 0.551, '90-99': 0.594}.
Avg inc acc top5: 0.8354545454545454.
Current acc top5: {'total': 0.768}.
Forgetting: 0.21099999999999997.
Cord metric: 0.56.
Old accuracy: 0.46, mean: 0.54.
New accuracy: 0.63, mean: 0.69.
Average Incremental Accuracy: 0.5603636363636363.
Label was: podnet_cnn_cifar100_50steps
Results done on 1 seeds: avg: 56.04, last: 47.0, forgetting: 21.1
Individual results avg: [56.04]
Individual results last: [47.0]
Individual results forget: [21.1]
Command was D:/go_to_D/ML/Final/LibContinual/run_trainer.py -minclearn --options config/podnet_cnn_cifar100.yaml --initial-increment 50 --increment 5 --fixed-memory --device 0 --label podnet_cnn_cifar100_50steps --save last
Time cost :  1493.9245491027832
